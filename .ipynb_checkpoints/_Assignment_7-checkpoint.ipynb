{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7\n",
    "# Implementing RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents :\n",
    "\n",
    "* Importing libraries\n",
    "* Data extraction\n",
    "* Data engineeering: converting characters into their respective integer representations\n",
    "* Applying Sequential model\n",
    "    * LSTM: number of neurons = 256\n",
    "    * Dropout: rate = 0.2\n",
    "* Mini-batch size = 128 (no. of samples per gradient update)\n",
    "* Number of epochs = 100\n",
    "* Number of characters to process in single go = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tvxNP6aOjZhY"
   },
   "outputs": [],
   "source": [
    "#Read the data, turn it into lower case\n",
    "data = open(\"shakespeare_input_mod.txt\").read().lower()\n",
    "#This get the set of characters used in the data and sorts them\n",
    "chars = sorted(list(set(data)))\n",
    "#Total number of characters used in the data\n",
    "totalChars = len(data)\n",
    "#Number of unique chars\n",
    "numberOfUniqueChars = len(chars)\n",
    "\n",
    "#This allows for characters to be represented by numbers\n",
    "CharsForids = {char:Id for Id, char in enumerate(chars)}\n",
    "\n",
    "#Converts numbers into the corresponding characters\n",
    "idsForChars = {Id:char for Id, char in enumerate(chars)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "tIGQ7qrmkSef",
    "outputId": "ade8d287-f299-48df-eb79-ede309ad6a20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#How many timesteps e.g how many characters we want to process in one go\n",
    "numberOfCharsToLearn = 1000\n",
    "\n",
    "#Since our timestep sequence represents a process for every 100 chars we omit\n",
    "#the first 100 chars so the loop runs a 100 less or there will be index out of\n",
    "#range\n",
    "counter = totalChars - numberOfCharsToLearn\n",
    "\n",
    "#Input data\n",
    "charX = []\n",
    "#output data\n",
    "y = []\n",
    "#This loops through all the characters in the data skipping the first 100\n",
    "# for i in range(0, counter, 1):\n",
    "for i in range(0, counter, 1):  \n",
    "    #This one goes from 0-100 so it gets 100 values starting from 0 and stops\n",
    "    #just before the 100th value\n",
    "    theInputChars = data[i:i+numberOfCharsToLearn]\n",
    "    #With no : you start with 0, and so you get the actual 100th value\n",
    "    #Essentially, the output Chars is the next char in line for those 100 chars\n",
    "    #in X\n",
    "    theOutputChars = data[i + numberOfCharsToLearn]\n",
    "    #Appends every 100 chars ids as a list into X\n",
    "    charX.append([CharsForids[char] for char in theInputChars])\n",
    "    #For every 100 values there is one y value which is the output\n",
    "    y.append(CharsForids[theOutputChars])\n",
    "\n",
    "#Len charX represents how many of those time steps we have\n",
    "#Our features are set to 1 because in the output we are only predicting 1 char\n",
    "#Finally numberOfCharsToLearn is how many character we process\n",
    "X = np.reshape(charX, (len(charX), numberOfCharsToLearn, 1))\n",
    "\n",
    "#This is done for normalization\n",
    "X = X/float(numberOfUniqueChars)\n",
    "\n",
    "#This sets it up for us so we can have a categorical(#feature) output format\n",
    "y = np_utils.to_categorical(y)\n",
    "print(y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "ma9d15TAkXnm",
    "outputId": "68e3a0f2-1f33-47f6-f52d-a005c1570b48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100000/100000 [==============================] - 658s 7ms/step - loss: 3.0274\n",
      "Epoch 2/100\n",
      "100000/100000 [==============================] - 659s 7ms/step - loss: 2.8022\n",
      "Epoch 3/100\n",
      "100000/100000 [==============================] - 658s 7ms/step - loss: 2.7009\n",
      "Epoch 4/100\n",
      "100000/100000 [==============================] - 654s 7ms/step - loss: 2.6294\n",
      "Epoch 5/100\n",
      "100000/100000 [==============================] - 659s 7ms/step - loss: 2.5770\n",
      "Epoch 6/100\n",
      " 27520/100000 [=======>......................] - ETA: 7:54 - loss: 2.5515"
     ]
    }
   ],
   "source": [
    "model = Sequential()   # The sequential model is linear stack of layer\n",
    "#Since we know the shape of our Data we can input the timestep and feature data\n",
    "#The number of timestep sequence are dealt with in the fit function\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))    # No. of neurons: 256, No. of timestep: X.shape[1], No. of features: X.shape[2]\n",
    "model.add(Dropout(0.2))\n",
    "#number of features on the output\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=100, batch_size=128)\n",
    "model.save_weights(\"weights.hdf5\")    #to save weights in the file\n",
    "#model.load_weights(\"weights.hdf5\")   #to load weights from the file\n",
    "\n",
    "randomVal = np.random.randint(0, len(charX)-1)\n",
    "randomStart = charX[randomVal]    # chossing any random character number for initiating character\n",
    "for i in range(500):\n",
    "    x = np.reshape(randomStart, (1, len(randomStart), 1))    # reshaping randomStart into the shape (1,len(randomStart)); order = 1\n",
    "    x = x/float(numberOfUniqueChars)\n",
    "    pred = model.predict(x)\n",
    "    index = np.argmax(pred)\n",
    "    randomStart.append(index)\n",
    "    randomStart = randomStart[1: len(randomStart)]\n",
    "print(\"\".join([idsForChars[value] for value in randomStart]))    #creating an empty string and appending it with words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jRVkw8a_VaWz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "#Assignment_7.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
